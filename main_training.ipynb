{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect distracted drivers\n",
    "*Cyril Pecoraro - June 1st 2018*\n",
    "\n",
    "The goal is to predict what the driver is doing based on pictures\n",
    "\n",
    "This project is from a [Kaggle competition : State Farm Distracted Driver Detection\n",
    "](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "There are 10 classes to predict:\n",
    "* c0: safe driving\n",
    "* c1: texting - right\n",
    "* c2: talking on the phone - right\n",
    "* c3: texting - left\n",
    "* c4: talking on the phone - left\n",
    "* c5: operating the radio\n",
    "* c6: drinking\n",
    "* c7: reaching behind\n",
    "* c8: hair and makeup\n",
    "* c9: talking to passenger\n",
    "\n",
    "### Explanation of my work\n",
    "\n",
    "I used a pre-trained VGG-16 Convolutional Neural Network as a base layer. I then removed the last layer (=top layer) and added a Dense layer with a softmax to output the classification. The optimization algorithm is Adam with a small learning rate: **/tofill/**  . \n",
    "\n",
    "\n",
    "The used the weights trained VGG-16 on the image-net dataset. All the layers beside the last **/tofill/**  were frozen. I fine tuned the model using the training set and a validation set (25% split)\n",
    "\n",
    "\n",
    "**/tofill/** epochs were used. The learning_rate, number of epochs and numbers of layers to freeze values were determined by several experiments, not shown here. \n",
    "\n",
    "\n",
    "The framework was run on a Google Cloud Virtual Machine with a Tesla K80 GPU.\n",
    "\n",
    "#### To predict:\n",
    "Use the script `predict.py [Model/model_name]` to obtain prediction and generate a .csv file for Kaggle.\n",
    "\n",
    "### Content:\n",
    "\n",
    "1. File loading and Preprocessing\n",
    "2. Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Loading and preprocessing\n",
    "\n",
    "First we can observe the distribution of samples per class. This is important as it is a classification problem. In order to avoid the [accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox), we don't want to have classes too imbalanced. Here the classes are pretty well balanced. All good !\n",
    "\n",
    "To load the files quickly, I use a multi-processor approach which allows to multiply by at least 5 the loading speed.\n",
    "\n",
    "The data are then preprocess according th VGG-16 requirements: input of size 224\\*224 and mean substraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = pd.read_csv('Data/driver_imgs_list.csv')\n",
    "\n",
    "img_list['class_type'] = img_list['classname'].str.extract('(\\d)',expand=False).astype(np.int)\n",
    "plt.figure()\n",
    "img_list.groupby(['class_type'])['subject'].count().plot(kind='bar',alpha=0.9, layout=(1,1), color='b')\n",
    "plt.title('class distribution')\n",
    "plt.xticks(range(0,11))\n",
    "plt.xlabel('Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_image(img_file, img_reshape_size):\n",
    "    \"\"\"Load an image\n",
    "    Args:\n",
    "    - img_file: image file\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    Return:\n",
    "    - img: openCV image\n",
    "    \"\"\"   \n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, img_reshape_size)\n",
    "    # Preprocess input according to VGG16\n",
    "    vgg16.preprocess_input(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_train_dataset(dataset_dir, img_reshape_size, nprocs=10):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - dataset_dir: path to the directory containing subdirectories of images\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    - nprocs:Number of processors to use\n",
    "    Return:\n",
    "    - X: numpy array with each image data as a row\n",
    "    - y: numpy array with each class as an integer for each image\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # Train dataset\n",
    "    for i in range(10):\n",
    "        path = os.path.join(dataset_dir, 'c'+str(i),'*.jpg')\n",
    "        files = glob.glob(path)\n",
    "\n",
    "        X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "        y.extend([i]*len(files))\n",
    "        print('folder train/c'+str(i), 'loaded')\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def convert_to_one_hot(Y, C=10):\n",
    "    \"\"\"Convert vector to one-hot representation\n",
    "    Args:\n",
    "    - Y: numpy vector\n",
    "    - C: number of different categories\n",
    "    Return:\n",
    "    - Y: numpy array with one-hot encoded\n",
    "    \"\"\"    \n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "    \n",
    "def plot_training_curves(model_history):\n",
    "    \"\"\"Prints accuracy and Loss training curves in 2 subplots\n",
    "    Args:\n",
    "    - model_history: Keras model.fit() object\n",
    "    Return:\n",
    "    /\n",
    "    \"\"\"\n",
    "    acc = model_history.history['acc']\n",
    "    val_acc = model_history.history['val_acc']\n",
    "    loss = model_history.history['loss']\n",
    "    val_loss = model_history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,1) \n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1,2,2) \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directories#  \n",
    "dataset_dir = 'Data'\n",
    "dataset_dir_train = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# Image sizes - Requirements of the CNN model\n",
    "img_reshape_size = (224,224)\n",
    "\n",
    "# Load train dataset\n",
    "print('Load dataset train')\n",
    "X_train_, y_train_ = load_train_dataset(dataset_dir_train, img_reshape_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a validation set. The chosen split is 25%, randomly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, \n",
    "                                                  test_size=0.25, \n",
    "                                                  random_state=42)\n",
    "\n",
    "# One-hot encoding of the target vector\n",
    "y_train = convert_to_one_hot(y_train)\n",
    "y_val = convert_to_one_hot(y_val)\n",
    "\n",
    "\n",
    "# Shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16_model(n_classes=10, n_layers_freeze=1, input_shape=(224, 224, 3), learning_rate=0.0001):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - n_classes: number of classes to predict for the classifier\n",
    "    - n_layers_freeze: number of last layers to freeze so that they are not trained again\n",
    "    Return:\n",
    "    - model: Keras model\n",
    "    \"\"\"\n",
    "    #Load the VGG model\n",
    "    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the layers except the last n_layers_freeze layers\n",
    "    for layer in vgg16_base.layers[:-n_layers_freeze]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential(name='VGG16-classifier')\n",
    "    model.add(vgg16_base)\n",
    "    model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "          optimizer=optimizers.Adam(lr=learning_rate),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "batch_size = 100\n",
    "n_epoch = 3\n",
    "learning_rate = 0.0001\n",
    "n_layers_freeze = 1\n",
    "\n",
    "# Create and compile model\n",
    "model = create_VGG16_model(n_layers_freeze=n_layers_freeze, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(x=X_train, y=y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=n_epoch,\n",
    "          shuffle=True,\n",
    "          verbose=10,\n",
    "          validation_data=(X_val, y_val),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict(X_val, batch_size=batch_size, verbose=1)\n",
    "score_val = log_loss(y_val, y_pred_val)\n",
    "print('Validation Log loss:', np.round(score_val,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can save it and exit this notebook.\n",
    "\n",
    "Use the script `predict.py [Model/model_name]` to obtain prediction and generate a .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and Exit\n",
    "if not os.path.isdir('Model'):\n",
    "    os.mkdir('Model')\n",
    "filename = 'VGG16_lr'+str(learning_rate)+'_freeze'+str(n_layers_freeze)+'_epochs'+str(n_epoch)+'.h5'\n",
    "sub_file = os.path.join('Model', 'submission_' + filename)\n",
    "model.save(filename)\n",
    "print('File', filename, 'saved')\n",
    "\n",
    "\n",
    "# Exit and kill kernel\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect distracted drivers\n",
    "*Cyril Pecoraro - June 1st 2018*\n",
    "\n",
    "The goal is to predict what the driver is doing based on pictures\n",
    "\n",
    "This project is from a [Kaggle competition : State Farm Distracted Driver Detection\n",
    "](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "There are 10 classes to predict:\n",
    "* c0: safe driving\n",
    "* c1: texting - right\n",
    "* c2: talking on the phone - right\n",
    "* c3: texting - left\n",
    "* c4: talking on the phone - left\n",
    "* c5: operating the radio\n",
    "* c6: drinking\n",
    "* c7: reaching behind\n",
    "* c8: hair and makeup\n",
    "* c9: talking to passenger\n",
    "\n",
    "### Explanation of my work\n",
    "\n",
    "I used a pre-trained VGG-16 Convolutional Neural Network as a base layer. I then removed the last layer (=top layer) and added a Dense layer with a softmax to output the classification. The optimization algorithm is Adam with a small learning rate: **/tofill/**  . \n",
    "\n",
    "\n",
    "The used the weights trained VGG-16 on the image-net dataset for the initializatzion. All the layers beside the last **/tofill/**  were frozen. I then performed data augmentation because the training set is not big. I fine tuned the model using the training set and a validation set (33% split)\n",
    "\n",
    "I used the Keras library as much as possible in this project as it allows to prototype quickly and surely. I used the dataflow and datagen functions to load the file and preprocess them while feeding them by batches directly to the model.\n",
    "\n",
    "\n",
    "**/tofill/** epochs were used. The learning_rate, number of epochs and numbers of layers to freeze values were determined by several experiments, not shown here. \n",
    "\n",
    "\n",
    "The framework was run on a Google Cloud Virtual Machine with a Tesla K80 GPU.\n",
    "\n",
    "#### To predict:\n",
    "* Run the last cell of this notebook\n",
    "* Or use the script `predict.py [Model/model_name]` with the desired model to obtain prediction and generate a .csv file for Kaggle.\n",
    "\n",
    "### Content:\n",
    "\n",
    "1. Model building\n",
    "2. Preprocessing and data augmentation\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config to adjust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "batch_size = 144 # Try 48; 72; 109; 144; 218; 327 depending on the computing power\n",
    "n_epoch = 20\n",
    "learning_rate = 0.0001\n",
    "n_layers_train = 3 # last n_layers_train layers will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpli\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, callbacks\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Flatten, Dense \n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we can observe the distribution of samples per class. This is important as it is a classification problem. In order to avoid the [accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox), we don't want to have classes too imbalanced. Here the classes are pretty well balanced. All good !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFoVJREFUeJzt3Xu03WV95/H3R26tQgUkIAQwXoIjtktkUmSktji03LSDOnUEp5I6OumsgaV2OY5op4OtY5edVbXaWmdhieIVUbFmRipQKlodLwmUcjEgERFiECIgiIgS/M4fv+cMm3BOzkly9s7leb/WOmvv/fye/fs+v5OT8zm/53fZqSokSf15zLYegCRp2zAAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBou5Dk95J8eRvW/2CS/9GePy/JDfO47r9LsrQ9n9ftTPLvk1wyX+tTX3bd1gOQtjdV9Y/A02frl+QtwNOq6ndnWd9J8zGuJIuA7wC7VdWGtu6PAh+dj/WrP+4BSGOSgf/HtN3yh1MTleSQJBcmWZ/kziR/NUO/dye5Ncm9Sa5I8ryRZUclWdWW3Z7kna39F5J8pK33h0lWJjlghvU/O8mVSX6U5BPAL4wsOzbJ2pHXb0zyvdb3hiTHJTkReDPwsiT3Jfnn1vfyJG9L8hXgfuApre3Vjyyfv0xyT5Lrkxw3suDmJL858votST7SXn6pPf6w1fxXG08pJXlu2+572uNzR5ZdnuStSb7StuWSJPvN9G+lnZ8BoIlJsgvwf4DvAouAhcD5M3RfCRwB7At8DPhkkqlf0u8G3l1VvwQ8FbigtS8FHg8cAjwB+E/AT6YZx+7A3wIfbuv/JPBvZxjz04EzgV+tqr2AE4Cbq+rzwJ8Cn6iqPavqWSNvewWwDNirbevGngPcBOwHnA1cmGTfGb4Po369Pe7dan51o7HuC3wOeA/D9r8T+FySJ4x0eznwSmB/YHfgv8yhrnZSBoAm6SjgIOANVfXjqnqgqqY9IFpVH6mqO6tqQ1W9A9iDh+flHwSelmS/qrqvqr420v4Ehnn5h6rqiqq6d5rVHw3sBvxFVT1YVZ9iCJzpPNRqH55kt6q6uaq+Pct2frCqrmtjf3Ca5XeM1P4EcAPwglnWORcvAG6sqg+32h8Hrgd+e6TPB6rqW1X1E4bgPGIe6moHZQBokg4Bvjt1AHNTkrw+yeo2lfFDhr/sp6YrXgUcBlzfpjle2No/DFwMnJ9kXZL/mWS3aVZ/EPC9euSdEKf7S52qWgO8DngLcEeS85McNMvwb51l+XS1Z1vnXBzEo7fjuwx7WlO+P/L8fmDPeairHZQBoEm6FTg0ySbPPmvz/W8E/h2wT1XtDdwDBKCqbqyq0ximMf4M+FSSx7W/qP+4qg4Hngu8EDh9mhK3AQuTZKTt0JnGU1Ufq6pfA54EVKtJez7tWza1fTPUXtee/xh47MiyJ27Gete1MY46FPjeLO9TpwwATdI3GH75vj3J49pB22Om6bcXsAFYD+ya5L8DvzS1MMnvJllQVT8HftiaH0ry/CS/0o413MswJfTQNOv/alv/a5LsmuQlDNNTj5Lk6Un+dZI9gAcYjilMrfN2YNEWnOmzf6u9W5KXAs8ALmrLrgJObcuWAL8z8r71wM+Bp8yw3ouAw5K8vG3Xy4DDGY67SI9iAGhiquohhvnopwG3AGuBl03T9WLg74BvMUxhPMAjp1VOBK5Lch/DAeFTq+oBhr+WP8Xwy3818EXgI2ykqn4GvAT4PeDuNoYLZxj2HsDbgR8wTJ/sz3D2DwwHjwHuTHLlJjf+kb4OLG7rfBvwO1V1Z1v2RwwHtu8G/pjhAPjUuO9v/b/SznI6eqPtupNhr+f1wJ3AfwVeWFU/2IyxqSPxA2EkqU/uAUhSpwwASeqUASBJnTIAJKlTBoAkdWq7vh30fvvtV4sWLdrWw5CkHcoVV1zxg6paMFu/7ToAFi1axKpVq7b1MCRph5Jk2lubbMwpIEnqlAEgSZ0yACSpUwaAJHVq1gBoH+H3hXZv9uuSvLa1v6V9TN5V7evkkfe8Kcma9vF5J4y0n9ja1iQ5azybJEmai7mcBbQBeH1VXZlkL+CKJJe2Ze+qqj8f7ZzkcOBU4JkMH1Dx90kOa4vfC/wWw10gVyZZUVXfnI8NkSRtnlkDoKpuY7iHO1X1oySreeQnDG3sFOD8qvop8J0ka3j4XutrquomgCTnt74GgCRtA5t1DCDJIuDZDPczBzgzydVJlifZp7Ut5JH3bl/b2mZq37jGsiSrkqxav3795gxPkrQZ5nwhWJI9gU8Dr6uqe5O8D3grw8fUvRV4B/AfaB/bt5Fi+rB51IcRVNU5wDkAS5Ys2eSHFRy0lZ+ium7d7H0kaWc1pwBoH6z9aeCjVXUhQFXdPrL8/Tz8sXNrGT78e8rBPPx5pzO1S5ImbC5nAQU4F1hdVe8caT9wpNuLgWvb8xUMn2m6R5InM3z03TeAlcDiJE9OsjvDgeIV87MZkqTNNZc9gGOAVwDXJLmqtb0ZOC3JEQzTODcDvw9QVdcluYDh4O4G4Iz2WbAkOZPh8153AZZX1XXzuC2SpM2wXX8m8JIlS2pTN4PzGIAkPVqSK6pqyWz9vBJYkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKRObdcfCq/tm9dhSDs29wAkqVPuAWiH1fseSO/br61nAOzA/AWwbfn9147OKSBJ6pQBIEmdMgAkqVMeA9gKzgFL2pG5ByBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU94KQpI2085yGxj3ACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnZg2AJIck+UKS1UmuS/La1r5vkkuT3Nge92ntSfKeJGuSXJ3kyJF1LW39b0yydHybJUmazVz2ADYAr6+qZwBHA2ckORw4C7isqhYDl7XXACcBi9vXMuB9MAQGcDbwHOAo4Oyp0JAkTd6sAVBVt1XVle35j4DVwELgFOC81u084EXt+SnAh2rwNWDvJAcCJwCXVtVdVXU3cClw4rxujSRpzjbrGECSRcCzga8DB1TVbTCEBLB/67YQuHXkbWtb20ztkqRtYM63gkiyJ/Bp4HVVdW+SGbtO01abaN+4zjKGqSMOPfTQuQ5P0oTtLLdD6Nmc9gCS7Mbwy/+jVXVha769Te3QHu9o7WuBQ0befjCwbhPtj1BV51TVkqpasmDBgs3ZFknSZpjLWUABzgVWV9U7RxatAKbO5FkKfHak/fR2NtDRwD1tiuhi4Pgk+7SDv8e3NknSNjCXKaBjgFcA1yS5qrW9GXg7cEGSVwG3AC9tyy4CTgbWAPcDrwSoqruSvBVY2fr9SVXdNS9bIUnabLMGQFV9menn7wGOm6Z/AWfMsK7lwPLNGaAkaTy8EliSOmUASFKn/EQwSTukrTkN1VNQB+4BSFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE7NGgBJlie5I8m1I21vSfK9JFe1r5NHlr0pyZokNyQ5YaT9xNa2JslZ878pkqTNMZc9gA8CJ07T/q6qOqJ9XQSQ5HDgVOCZ7T1/nWSXJLsA7wVOAg4HTmt9JUnbyK6zdaiqLyVZNMf1nQKcX1U/Bb6TZA1wVFu2pqpuAkhyfuv7zc0esSRpXmzNMYAzk1zdpoj2aW0LgVtH+qxtbTO1S5K2kS0NgPcBTwWOAG4D3tHaM03f2kT7oyRZlmRVklXr16/fwuFJkmazRQFQVbdX1UNV9XPg/Tw8zbMWOGSk68HAuk20T7fuc6pqSVUtWbBgwZYMT5I0B1sUAEkOHHn5YmDqDKEVwKlJ9kjyZGAx8A1gJbA4yZOT7M5woHjFlg9bkrS1Zj0InOTjwLHAfknWAmcDxyY5gmEa52bg9wGq6rokFzAc3N0AnFFVD7X1nAlcDOwCLK+q6+Z9ayRJczaXs4BOm6b53E30fxvwtmnaLwIu2qzRSZLGxiuBJalTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ2aNQCSLE9yR5JrR9r2TXJpkhvb4z6tPUnek2RNkquTHDnynqWt/41Jlo5ncyRJczWXPYAPAidu1HYWcFlVLQYua68BTgIWt69lwPtgCAzgbOA5wFHA2VOhIUnaNmYNgKr6EnDXRs2nAOe15+cBLxpp/1ANvgbsneRA4ATg0qq6q6ruBi7l0aEiSZqgLT0GcEBV3QbQHvdv7QuBW0f6rW1tM7VLkraR+T4InGnaahPtj15BsizJqiSr1q9fP6+DkyQ9bEsD4PY2tUN7vKO1rwUOGel3MLBuE+2PUlXnVNWSqlqyYMGCLRyeJGk2WxoAK4CpM3mWAp8daT+9nQ10NHBPmyK6GDg+yT7t4O/xrU2StI3sOluHJB8HjgX2S7KW4WyetwMXJHkVcAvw0tb9IuBkYA1wP/BKgKq6K8lbgZWt359U1cYHliVJEzRrAFTVaTMsOm6avgWcMcN6lgPLN2t0kqSx8UpgSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1FYFQJKbk1yT5Kokq1rbvkkuTXJje9yntSfJe5KsSXJ1kiPnYwMkSVtmPvYAnl9VR1TVkvb6LOCyqloMXNZeA5wELG5fy4D3zUNtSdIWGscU0CnAee35ecCLRto/VIOvAXsnOXAM9SVJc7C1AVDAJUmuSLKstR1QVbcBtMf9W/tC4NaR965tbZKkbWDXrXz/MVW1Lsn+wKVJrt9E30zTVo/qNATJMoBDDz10K4cnSZrJVu0BVNW69ngH8BngKOD2qamd9nhH674WOGTk7QcD66ZZ5zlVtaSqlixYsGBrhidJ2oQtDoAkj0uy19Rz4HjgWmAFsLR1Wwp8tj1fAZzezgY6GrhnaqpIkjR5WzMFdADwmSRT6/lYVX0+yUrggiSvAm4BXtr6XwScDKwB7gdeuRW1JUlbaYsDoKpuAp41TfudwHHTtBdwxpbWkyTNL68ElqROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTk08AJKcmOSGJGuSnDXp+pKkwUQDIMkuwHuBk4DDgdOSHD7JMUiSBpPeAzgKWFNVN1XVz4DzgVMmPAZJErDrhOstBG4deb0WeM5ohyTLgGXt5X1JbtiKevsBP5hpYbIVa7a+9a2/PdfvedsBnj6XTpMOgOk2ux7xouoc4Jx5KZasqqol87Eu61vf+jtO/Z63far+XPpNegpoLXDIyOuDgXUTHoMkickHwEpgcZInJ9kdOBVYMeExSJKY8BRQVW1IciZwMbALsLyqrhtjyXmZSrK+9a2/w9XvedvnXD9VNXsvSdJOxyuBJalTBoAkdcoAkKROTfo6gLFK8i8YrixeyHB9wTpgRVWt3qYDm5C2/QuBr1fVfSPtJ1bV5ydQ/yigqmplu8XHicD1VXXRuGtPM5YPVdXpk647Uv/XGK58v7aqLhlzrecAq6vq3iS/CJwFHAl8E/jTqrpnzPVfA3ymqm6dtfN46k+dUbiuqv4+ycuB5wKrgXOq6sEJjOGpwIsZTnPfANwIfHzc3/uttdMcBE7yRuA0httLrG3NBzP8YJxfVW/fhmN7ZVV9YMw1XgOcwfBDfwTw2qr6bFt2ZVUdOeb6ZzPc42lX4FKGK7wvB34TuLiq3jbG2hufShzg+cA/AFTVvxlX7ZExfKOqjmrP/yPDv8VngOOB/z3On78k1wHPamfZnQPcD3wKOK61v2RctVv9e4AfA98GPg58sqrWj7PmRvU/yvBz91jgh8CewIUM25+qWjrm+q8Bfhv4InAycBVwN0Mg/Oequnyc9bdKVe0UX8C3gN2mad8duHEbj+2WCdS4BtizPV8ErGIIAYB/mlD9XRj+E94L/FJr/0Xg6jHXvhL4CHAs8Bvt8bb2/Dcm9G/8TyPPVwIL2vPHAdeMufbq0e/FRsuumsS2M0wnHw+cC6wHPg8sBfaaQP2r2+OuwO3ALu11xv2z1+pcM1LzscDl7fmhk/i/tzVfO9MU0M+Bg4DvbtR+YFs2VkmunmkRcMC46zP8AN4HUFU3JzkW+FSSJzH9LTjm24aqegi4P8m3q+reNpafJBn3938J8FrgD4E3VNVVSX5SVV8cc91Rj0myD8MvwlT7C7iqfpxkw5hrXzuyl/nPSZZU1aokhwFjn/5gmPb7OXAJcEmS3Rj2Bk8D/hxYMOb6j2nTQI9j+AX8eOAuYA9gtzHXnrIr8FCruRdAVd3SvhfbrZ0pAF4HXJbkRh6+4dyhwNOAMydQ/wDgBIZdv1EB/u8E6n8/yRFVdRVAVd2X5IXAcuBXJlD/Z0keW1X3A/9yqjHJ4xlzALdfPu9K8sn2eDuT/9l+PHAFw793JXliVX0/yZ6MP4BfDbw7yX9juAHZV5PcyvD/4NVjrg0bbV8Nc+4rgBXtmMS4nQtcz7AH+ofAJ5PcBBzNMCU8bn8DrEzyNeDXgT8DSLKAIYi2WzvNMQCAJI9hOPC2kOGHci2wsv1lOu7a5wIfqKovT7PsY1X18jHXP5jhr/DvT7PsmKr6ypjr71FVP52mfT/gwKq6Zpz1N6r5AuCYqnrzpGpuYiyPBQ6oqu9MoNZewFMYwm9tVd0+7pqt7mFV9a1J1NrEGA4CqKp1SfZmOPZ0S1V9Y0L1nwk8g+Gg//WTqDkfdqoAkCTNndcBSFKnDABJ6pQBIDVJnpjk/CTfTvLNJBclOSzJtdt6bNI47ExnAUlbLEkYLtw6r6pObW1HMJlTeKVtwj0AafB84MGq+l9TDe2U2v9/e4Mki5L8Y5Ir29dzW/uBSb6U5Kok1yZ5XpJdknywvb4myR9MfpOkTXMPQBr8MsN5/JtyB/BbVfVAksUMtz1YArycdruLJFNXQx8BLKyqXwZopyZK2xUDQJq73YC/alNDDwGHtfaVwPJ21efftiuRbwKekuQvgc8xXCUrbVecApIG1zFyBfMM/oDhXjPPYvjLf3eAqvoSwxWg3wM+nOT0qrq79buc4cZwfzOeYUtbzgCQBv8A7NHu5AlAkl8FnjTS5/HAbe3WE69guPUA7X5Ld1TV+xluS3BkuwL6MVX1aeCPGG7PLG1XnAKSGO5mluTFwF8kOQt4ALiZ4R5TU/4a+HSSlwJfYLgFMgx3H31DkgeB+4DTGW5H8oF2exKAN419I6TN5K0gJKlTTgFJUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOvX/AOkUUHhEqmVXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff06b92cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_list = pd.read_csv('Data/driver_imgs_list.csv')\n",
    "\n",
    "img_list['class_type'] = img_list['classname'].str.extract('(\\d)',expand=False).astype(np.int)\n",
    "plt.figure()\n",
    "img_list.groupby(['class_type'])['subject'].count().plot(kind='bar',alpha=0.9, layout=(1,1), color='b')\n",
    "plt.title('class distribution')\n",
    "plt.xticks(range(0,11))\n",
    "plt.xlabel('Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "To load the files quickly, I use a multi-processor approach which allows to multiply by at least 5 the loading speed.\n",
    "\n",
    "The data is split in a train/validation set. Then I use data augmentation with transformation to increase the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_file, img_reshape_size):\n",
    "    \"\"\"Load an image\n",
    "    Args:\n",
    "    - img_file: image file\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    Return:\n",
    "    - img: openCV image\n",
    "    \"\"\"   \n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, img_reshape_size)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #img = preprocessing_function=vgg16.preprocess_input(img)\n",
    "  \n",
    "    return img\n",
    "\n",
    "def load_train_dataset(dataset_dir, img_reshape_size, nprocs=10):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - dataset_dir: path to the directory containing subdirectories of images\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    - nprocs:Number of processors to use\n",
    "    Return:\n",
    "    - X: numpy array with each image data as a row\n",
    "    - y: numpy array with each class as an integer for each image\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # Train dataset\n",
    "    for i in range(10):\n",
    "        path = os.path.join(dataset_dir, 'c'+str(i),'*.jpg')\n",
    "        files = glob.glob(path)\n",
    "\n",
    "        X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "        y.extend([i]*len(files))\n",
    "        print('folder train/c'+str(i), 'loaded')\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset train\n",
      "folder train/c0 loaded\n",
      "folder train/c1 loaded\n",
      "folder train/c2 loaded\n",
      "folder train/c3 loaded\n",
      "folder train/c4 loaded\n",
      "folder train/c5 loaded\n",
      "folder train/c6 loaded\n",
      "folder train/c7 loaded\n",
      "folder train/c8 loaded\n",
      "folder train/c9 loaded\n",
      "X_train shape: (15696, 224, 224, 3)\n",
      "y_train shape: (15696, 10)\n",
      "X_val shape: (6728, 224, 224, 3)\n",
      "y_val shape: (6728, 10)\n"
     ]
    }
   ],
   "source": [
    "# Image sizes - Requirements of the CNN model\n",
    "img_reshape_size = (224,224)\n",
    "\n",
    "# Working directories\n",
    "dataset_dir = 'Data'\n",
    "dataset_dir_train = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# Load train dataset\n",
    "print('Load dataset train')\n",
    "X_train_, y_train_ = load_train_dataset(dataset_dir_train, img_reshape_size)\n",
    "X_train_[:, :, :, 0] -= 103.939\n",
    "X_train_[:, :, :, 1] -= 116.779\n",
    "X_train_[:, :, :, 2] -= 123.68   \n",
    "        \n",
    "        # Creation of a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, \n",
    "                                                  test_size=0.3, \n",
    "                                                  random_state=35)\n",
    "\n",
    "# One hot encoding of the classes\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)#\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "# Image Data Generator for preprocessing and data augmentation\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        rotation_range=8,\n",
    "        fill_mode='nearest'\n",
    "                                        )\n",
    "\n",
    "train_generator = train_datagen.flow(x=X_train, y=y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model building\n",
    "\n",
    "I use the VGG16 implementation and I remove the last layer to add a layer that suits my needs. In this case it is a 10 Fully connected layer so that I can use the CNN as a 10-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16_model(n_classes=10, n_layers_train=2, learning_rate=0.0001):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - n_classes: number of classes to predict for the classifier\n",
    "    - n_layers_train: number of last layers to train. The other layers will be frozen.\n",
    "    Return:\n",
    "    - model: Keras model\n",
    "    \"\"\"\n",
    "    #Load the VGG model\n",
    "    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=True, input_shape=(224,224,3))    \n",
    "\n",
    "    # Remove top layer and connect custom dense layer\n",
    "    fc2 = vgg16_base.get_layer('fc2').output\n",
    "    mypredictions = Dense(n_classes, activation='softmax', name='mypredictions')(fc2)\n",
    "    model = Model(inputs=vgg16_base.input, outputs=mypredictions)\n",
    "    \n",
    "    model.summary()\n",
    "    # Freeze the layers except the last n_layers_train layers\n",
    "    for layer in model.layers[:-n_layers_train]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[:]:\n",
    "        print(layer, layer.trainable)    \n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizers.Adam(lr=learning_rate),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "mypredictions (Dense)        (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 134,301,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<keras.engine.input_layer.InputLayer object at 0x7eff06b92358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff06b92748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff06b92780> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7eff06b922b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff04942860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff04973e48> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7eff0497e3c8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff04931d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff049318d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff049bedd8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7eff049ab080> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff04961d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff049d1f98> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7eff049085c0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7effa9cb3f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7effa9ccdf60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7effa9a9f2e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7effa9ab5ac8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7effa9c59fd0> False\n",
      "<keras.layers.core.Flatten object at 0x7effa9c86080> False\n",
      "<keras.layers.core.Dense object at 0x7effa9c86390> True\n",
      "<keras.layers.core.Dense object at 0x7effa9c2f8d0> True\n",
      "<keras.layers.core.Dense object at 0x7effa9bf67b8> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "mypredictions (Dense)        (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 119,586,826\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and compile model\n",
    "model = create_VGG16_model(n_layers_train=n_layers_train, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 277s 3s/step - loss: 0.5615 - acc: 0.8248 - val_loss: 0.0736 - val_acc: 0.9793\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0798 - acc: 0.9733 - val_loss: 0.0805 - val_acc: 0.9729\n",
      "Epoch 3/20\n",
      "109/109 [==============================] - 257s 2s/step - loss: 0.0615 - acc: 0.9801 - val_loss: 0.0348 - val_acc: 0.9902\n",
      "Epoch 4/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0439 - acc: 0.9854 - val_loss: 0.0487 - val_acc: 0.9848\n",
      "Epoch 5/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0455 - acc: 0.9850 - val_loss: 0.0448 - val_acc: 0.9857\n",
      "Epoch 6/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0357 - acc: 0.9885 - val_loss: 0.0307 - val_acc: 0.9911\n",
      "Epoch 7/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0373 - acc: 0.9890 - val_loss: 0.0236 - val_acc: 0.9930\n",
      "Epoch 8/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0416 - acc: 0.9880 - val_loss: 0.0158 - val_acc: 0.9960\n",
      "Epoch 9/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.0175 - val_acc: 0.9952\n",
      "Epoch 10/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0284 - val_acc: 0.9914\n",
      "Epoch 11/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0276 - acc: 0.9917 - val_loss: 0.0305 - val_acc: 0.9936\n",
      "Epoch 12/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0319 - acc: 0.9907 - val_loss: 0.0176 - val_acc: 0.9945\n",
      "Epoch 13/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.0197 - val_acc: 0.9958\n",
      "Epoch 14/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0267 - acc: 0.9910 - val_loss: 0.0266 - val_acc: 0.9938\n",
      "Epoch 15/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.0245 - val_acc: 0.9960\n",
      "Epoch 16/20\n",
      "109/109 [==============================] - 254s 2s/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0149 - val_acc: 0.9964\n",
      "Epoch 17/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0196 - acc: 0.9939 - val_loss: 0.0492 - val_acc: 0.9896\n",
      "Epoch 18/20\n",
      "109/109 [==============================] - 256s 2s/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.0174 - val_acc: 0.9967\n",
      "Epoch 19/20\n",
      "109/109 [==============================] - 255s 2s/step - loss: 0.0200 - acc: 0.9941 - val_loss: 0.0282 - val_acc: 0.9957\n",
      "Epoch 20/20\n",
      "109/109 [==============================] - 254s 2s/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.0322 - val_acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "          write_graph=False, write_images=False)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model_history = model.fit_generator(train_generator,\n",
    "                                    validation_data=(X_val, y_val),\n",
    "                                    shuffle=True,\n",
    "                                    epochs=n_epoch,\n",
    "                                    steps_per_epoch=np.ceil(X_train.shape[0]//batch_size),\n",
    "                                    callbacks=[tbCallBack],\n",
    "                                    verbose=1,\n",
    "                                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can save it for further predictions.\n",
    "\n",
    "Use the script `predict.py [Model/model_name]` to obtain predictions and generate a .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File VGG16_lr0.0001_train3_epochs20.h5 saved\n"
     ]
    }
   ],
   "source": [
    "# Save model and Exit\n",
    "if not os.path.isdir('Model'):\n",
    "    os.mkdir('Model')\n",
    "filename = 'VGG16_lr'+str(learning_rate)+'_train'+str(n_layers_train)+'_epochs'+str(n_epoch)+'.h5'\n",
    "\n",
    "model_file = os.path.join('Model', filename)\n",
    "model.save(model_file)\n",
    "print('File', filename, 'saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict and submit\n",
    "\n",
    "Using the model that was trained in this notebook, the script is going to predict the results for the test set and directly submit the results to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#delete when no longer needed\n",
    "del X_train, X_val, X_train_, model_history\n",
    "#collect residual garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file='Model/VGG16_lr0.0001_train3_epochs20.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with model Model/VGG16_lr0.0001_train3_epochs20.h5\n",
      "Load dataset test\n",
      "Dataset test loaded\n",
      "X_test shape: (79726, 224, 224, 3)\n",
      "79726/79726 [==============================] - 771s 10ms/step\n",
      "File VGG16_lr0.0001_train3_epochs20_2018-07-06-21-02 saved\n"
     ]
    }
   ],
   "source": [
    "%run -i 'predict.py' $model_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to State Farm Distracted Driver Detection"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c state-farm-distracted-driver-detection -f \"$(ls -t Output/submission-* | head -1)\" -m \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

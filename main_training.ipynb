{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect distracted drivers\n",
    "*Cyril Pecoraro - June 1st 2018*\n",
    "\n",
    "The goal is to predict what the driver is doing based on pictures\n",
    "\n",
    "This project is from a [Kaggle competition : State Farm Distracted Driver Detection\n",
    "](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "There are 10 classes to predict:\n",
    "* c0: safe driving\n",
    "* c1: texting - right\n",
    "* c2: talking on the phone - right\n",
    "* c3: texting - left\n",
    "* c4: talking on the phone - left\n",
    "* c5: operating the radio\n",
    "* c6: drinking\n",
    "* c7: reaching behind\n",
    "* c8: hair and makeup\n",
    "* c9: talking to passenger\n",
    "\n",
    "### Explanation of my work\n",
    "\n",
    "I used a pre-trained VGG-16 Convolutional Neural Network as a base layer. I then removed the last layer (=top layer) and added a Dense layer with a softmax to output the classification. The optimization algorithm is Adam with a small learning rate: **/tofill/**  . \n",
    "\n",
    "\n",
    "The used the weights trained VGG-16 on the image-net dataset for the initializatzion. All the layers beside the last **/tofill/**  were frozen. I then performed data augmentation because the training set is not big. I fine tuned the model using the training set and a validation set (33% split)\n",
    "\n",
    "I used the Keras library as much as possible in this project as it allows to prototype quickly and surely. I used the dataflow and datagen functions to load the file and preprocess them while feeding them by batches directly to the model.\n",
    "\n",
    "\n",
    "**/tofill/** epochs were used. The learning_rate, number of epochs and numbers of layers to freeze values were determined by several experiments, not shown here. \n",
    "\n",
    "\n",
    "The framework was run on a Google Cloud Virtual Machine with a Tesla K80 GPU.\n",
    "\n",
    "#### To predict:\n",
    "* Run the last cell of this notebook\n",
    "* Or use the script `predict.py [Model/model_name]` with the desired model to obtain prediction and generate a .csv file for Kaggle.\n",
    "\n",
    "### Content:\n",
    "\n",
    "1. Model building\n",
    "2. Preprocessing and data augmentation\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config to adjust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "batch_size = 144 # Try 48; 72; 109; 144; 218; 327 depending on the computing power\n",
    "n_epoch = 20\n",
    "learning_rate = 0.00001\n",
    "n_layers_train = 2 # last n_layers_train layers will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpli\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, callbacks\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense \n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we can observe the distribution of samples per class. This is important as it is a classification problem. In order to avoid the [accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox), we don't want to have classes too imbalanced. Here the classes are pretty well balanced. All good !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f14a96cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_list = pd.read_csv('Data/driver_imgs_list.csv')\n",
    "\n",
    "img_list['class_type'] = img_list['classname'].str.extract('(\\d)',expand=False).astype(np.int)\n",
    "plt.figure()\n",
    "img_list.groupby(['class_type'])['subject'].count().plot(kind='bar',alpha=0.9, layout=(1,1), color='b')\n",
    "plt.title('class distribution')\n",
    "plt.xticks(range(0,11))\n",
    "plt.xlabel('Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "To load the files quickly, I use a multi-processor approach which allows to multiply by at least 5 the loading speed.\n",
    "\n",
    "The data is split in a train/validation set. Then I use data augmentation with transformation to increase the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_file, img_reshape_size):\n",
    "    \"\"\"Load an image\n",
    "    Args:\n",
    "    - img_file: image file\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    Return:\n",
    "    - img: openCV image\n",
    "    \"\"\"   \n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, img_reshape_size)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #img = preprocessing_function=vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def load_train_dataset(dataset_dir, img_reshape_size, nprocs=10):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - dataset_dir: path to the directory containing subdirectories of images\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    - nprocs:Number of processors to use\n",
    "    Return:\n",
    "    - X: numpy array with each image data as a row\n",
    "    - y: numpy array with each class as an integer for each image\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # Train dataset\n",
    "    for i in range(10):\n",
    "        path = os.path.join(dataset_dir, 'c'+str(i),'*.jpg')\n",
    "        files = glob.glob(path)\n",
    "\n",
    "        X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "        y.extend([i]*len(files))\n",
    "        print('folder train/c'+str(i), 'loaded')\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset train\n",
      "folder train/c0 loaded\n",
      "folder train/c1 loaded\n",
      "folder train/c2 loaded\n",
      "folder train/c3 loaded\n",
      "folder train/c4 loaded\n",
      "folder train/c5 loaded\n",
      "folder train/c6 loaded\n",
      "folder train/c7 loaded\n",
      "folder train/c8 loaded\n",
      "folder train/c9 loaded\n",
      "X_train shape: (15696, 224, 224, 3)\n",
      "y_train shape: (15696, 10)\n",
      "X_val shape: (6728, 224, 224, 3)\n",
      "y_val shape: (6728, 10)\n"
     ]
    }
   ],
   "source": [
    "# Image sizes - Requirements of the CNN model\n",
    "img_reshape_size = (224,224)\n",
    "\n",
    "# Working directories\n",
    "dataset_dir = 'Data'\n",
    "dataset_dir_train = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# Load train dataset\n",
    "print('Load dataset train')\n",
    "X_train_, y_train_ = load_train_dataset(dataset_dir_train, img_reshape_size)\n",
    "\n",
    "# Creation of a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, \n",
    "                                                  test_size=0.3, \n",
    "                                                  random_state=35)\n",
    "\n",
    "# One hot encoding of the classes\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)# 1-hot encoding done later\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)# 1-hot encoding done later\n",
    "\n",
    "X_val = vgg16.preprocess_input(X_val)\n",
    "\n",
    "# Image Data Generator for preprocessing and data augmentation\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "        preprocessing_function=vgg16.preprocess_input,\n",
    "        rotation_range=40,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "#val_datagen = image.ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow(x=X_train, y=y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=42)\n",
    "\n",
    "#val_generator = val_datagen.flow(x=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model building\n",
    "\n",
    "I use the VGG16 implementation and I remove the last layer to add a layer that suits my needs. In this case it is a 10 Fully connected layer so that I can use the CNN as a 10-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16_model(n_classes=10, n_layers_train=2, input_shape=(224, 224, 3), learning_rate=0.0001):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - n_classes: number of classes to predict for the classifier\n",
    "    - n_layers_train: number of last layers to train. The other layers will be frozen.\n",
    "    Return:\n",
    "    - model: Keras model\n",
    "    \"\"\"\n",
    "    #Load the VGG model\n",
    "    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=True, input_shape=input_shape)    \n",
    "\n",
    "    # Remove top dense layer\n",
    "    vgg16_base.layers.pop()\n",
    "    vgg16_base.outputs = [vgg16_base.layers[-1].output]\n",
    "    vgg16_base.layers[-1].outbound_nodes = []\n",
    "    \n",
    "    # Freeze the layers except the last n_layers_train layers\n",
    "    for layer in vgg16_base.layers[:-n_layers_train-1]:\n",
    "        layer.trainable = False\n",
    "    for layer in vgg16_base.layers[:-1]:\n",
    "        print(layer, layer.trainable)    \n",
    "    vgg16_base.summary()\n",
    "    \n",
    "    # Create custom classification model\n",
    "    model = Sequential(name='VGG16-classifier')\n",
    "    model.add(vgg16_base)\n",
    "    model.add(Dense(n_classes, activation='softmax', name='mycustompredictions'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizers.Adam(lr=learning_rate),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f8f12841fd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f12841198> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f12841a90> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f8f12841080> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f12872198> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f12832550> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f8f1285d6a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f128bd160> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f91dfbc88> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f128ac748> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f8f12863c50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f128527f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f128d4390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f128276d8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f8f12808630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8f12799d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8fad3ea278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f8fad3fefd0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f8fad3a4f28> False\n",
      "<keras.layers.core.Flatten object at 0x7f8fad3d00b8> True\n",
      "<keras.layers.core.Dense object at 0x7f8fad3d0320> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 119,545,856\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                multiple                  134260544 \n",
      "_________________________________________________________________\n",
      "mycustompredictions (Dense)  (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 119,586,826\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and compile model\n",
    "model = create_VGG16_model(n_layers_train=n_layers_train, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2083652712829269316\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11171915367\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2831067841911907429\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - 279s 3s/step - loss: 1.1474 - acc: 0.6511 - val_loss: 0.3069 - val_acc: 0.9044\n",
      "Epoch 2/20\n",
      " 90/109 [=======================>......] - ETA: 31s - loss: 0.2229 - acc: 0.9431"
     ]
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "model_history = model.fit_generator(train_generator,\n",
    "                                    validation_data=(X_val, y_val),\n",
    "                                    shuffle=True,\n",
    "                                    epochs=n_epoch,\n",
    "                                    steps_per_epoch=np.ceil(X_train.shape[0]//batch_size),\n",
    "                                    #callbacks=[tbCallBack],\n",
    "                                    verbose=1,\n",
    "                                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can save it for further predictions.\n",
    "\n",
    "Use the script `predict.py [Model/model_name]` to obtain predictions and generate a .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and Exit\n",
    "if not os.path.isdir('Model'):\n",
    "    os.mkdir('Model')\n",
    "filename = 'VGG16_lr'+str(learning_rate)+'_train'+str(n_layers_train)+'_epochs'+str(n_epoch)+'.h5'\n",
    "model_file = os.path.join('Model', filename)\n",
    "model.save(model_file)\n",
    "print('File', filename, 'saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict and submit\n",
    "\n",
    "Using the model that was trained in this notebook, the script is going to predict the results for the test set and directly submit the results to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'predict.py' $model_file\n",
    "!kaggle competitions submit -c state-farm-distracted-driver-detection -f \"$(ls -t Output/submission-* | head -1)\" -m \".\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect distracted drivers\n",
    "*Cyril Pecoraro - June 1st 2018*\n",
    "\n",
    "The goal is to predict what the driver is doing based on pictures\n",
    "\n",
    "This project is from a [Kaggle competition : State Farm Distracted Driver Detection\n",
    "](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "There are 10 classes to predict:\n",
    "* c0: safe driving\n",
    "* c1: texting - right\n",
    "* c2: talking on the phone - right\n",
    "* c3: texting - left\n",
    "* c4: talking on the phone - left\n",
    "* c5: operating the radio\n",
    "* c6: drinking\n",
    "* c7: reaching behind\n",
    "* c8: hair and makeup\n",
    "* c9: talking to passenger\n",
    "\n",
    "### Explanation of my work\n",
    "\n",
    "I used a pre-trained VGG-16 Convolutional Neural Network as a base layer. I then removed the last layer (=top layer) and added a Dense layer with a softmax to output the classification. The optimization algorithm is Adam with a small learning rate: 0.0001. \n",
    "\n",
    "\n",
    "The used the weights trained VGG-16 on the image-net dataset for the initializatzion. All the layers beside the last 3 layers were frozen (last 3 layers are fully connected layers). I then performed data augmentation because the training set is not big. I fine tuned the model using the training set and a validation set (30% split)\n",
    "\n",
    "I used the Keras library as much as possible in this project as it allows to prototype quickly and surely. I used the dataflow and datagen functions to load the file and preprocess them while feeding them by batches directly to the model.\n",
    "\n",
    "\n",
    "5 epochs only were used. The learning_rate, number of epochs and numbers of layers to freeze values were determined by several experiments, not shown here. \n",
    "\n",
    "\n",
    "The framework was run on a Google Cloud Virtual Machine with a Tesla K80 GPU.\n",
    "#### Result:\n",
    "* Public Leaderboard: 1.19 (top 40%)\n",
    "* Private Leaderboard 0.86562 (top 30%)\n",
    "\n",
    "\n",
    "\n",
    "#### To predict:\n",
    "* Run the last cell of this notebook\n",
    "* Or use the script `predict.py [Model/model_name]` with the desired model to obtain prediction and generate a .csv file for Kaggle.\n",
    "\n",
    "### Content:\n",
    "\n",
    "1. Model building\n",
    "2. Preprocessing and data augmentation\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config to adjust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params \n",
    "batch_size = 144 # Try 48; 72; 109; 144; 218; 327 depending on the computing power\n",
    "n_epoch = 5\n",
    "learning_rate = 0.0001\n",
    "n_layers_train = 3 # last n_layers_train layers will be trained\n",
    "data_augmentation = False # True to augment image by transformation in the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpli\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, callbacks\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Flatten, Dense \n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-74b263da4e4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_datagen = image.ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow(x=X_test, y=y_test, batch_size=150)\n",
    "\n",
    "model.pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we can observe the distribution of samples per class. This is important as it is a classification problem. In order to avoid the [accuracy paradox](https://en.wikipedia.org/wiki/Accuracy_paradox), we don't want to have classes too imbalanced. Here the classes are pretty well balanced. All good !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7488657eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_list = pd.read_csv('Data/driver_imgs_list.csv')\n",
    "\n",
    "img_list['class_type'] = img_list['classname'].str.extract('(\\d)',expand=False).astype(np.int)\n",
    "plt.figure()\n",
    "img_list.groupby(['class_type'])['subject'].count().plot(kind='bar',alpha=0.9, layout=(1,1), color='b')\n",
    "plt.title('class distribution')\n",
    "plt.xticks(range(0,11))\n",
    "plt.xlabel('Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "To load the files quickly, I use a multi-processor approach which allows to multiply by at least 5 the loading speed.\n",
    "\n",
    "The data is split in a train/validation set. Then I can use data augmentation with transformation to increase the size of the training set (not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"Preprocess an image according to VGG16 imagenet requirement (mean substraction)\n",
    "    Args:\n",
    "    - img: image in BGR format with shape: [w,h,channel]\n",
    "    Return:\n",
    "    - img: preprocessed image\n",
    "    \"\"\"   \n",
    "    img = img.astype(np.float16)\n",
    "    img[:, :, 0] -= 103.939\n",
    "    img[:, :, 1] -= 116.779\n",
    "    img[:, :, 2] -= 123.68\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_image(img_file, img_reshape_size):\n",
    "    \"\"\"Load an image\n",
    "    Args:\n",
    "    - img_file: image file\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    Return:\n",
    "    - img: image resized and preprocessed according to VGG16 standards\n",
    "    \"\"\"   \n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, img_reshape_size)\n",
    "    img = preprocess_image(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_train_dataset(dataset_dir, img_reshape_size, nprocs=10):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - dataset_dir: path to the directory containing subdirectories of images\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    - nprocs:Number of processors to use\n",
    "    Return:\n",
    "    - X: numpy array with each image data as a row\n",
    "    - y: numpy array with each class as an integer for each image\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # Train dataset\n",
    "    for i in range(10):\n",
    "        path = os.path.join(dataset_dir, 'c'+str(i),'*.jpg')\n",
    "        files = glob.glob(path)\n",
    "\n",
    "        X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "        y.extend([i]*len(files))\n",
    "        print('folder train/c'+str(i), 'loaded')\n",
    "\n",
    "    X = np.asarray(X, dtype=np.float16)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset train\n",
      "folder train/c0 loaded\n",
      "folder train/c1 loaded\n",
      "folder train/c2 loaded\n",
      "folder train/c3 loaded\n",
      "folder train/c4 loaded\n",
      "folder train/c5 loaded\n",
      "folder train/c6 loaded\n",
      "folder train/c7 loaded\n",
      "folder train/c8 loaded\n",
      "folder train/c9 loaded\n",
      "X_train shape: (15696, 224, 224, 3)\n",
      "y_train shape: (15696, 10)\n",
      "X_val shape: (6728, 224, 224, 3)\n",
      "y_val shape: (6728, 10)\n"
     ]
    }
   ],
   "source": [
    "# Image sizes - Requirements of the CNN model\n",
    "img_reshape_size = (224,224)\n",
    "\n",
    "# Working directories\n",
    "dataset_dir = 'Data'\n",
    "dataset_dir_train = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# Load train dataset\n",
    "print('Load dataset train')\n",
    "X_train_, y_train_ = load_train_dataset(dataset_dir_train, img_reshape_size) \n",
    "        \n",
    "# Creation of a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, \n",
    "                                                  test_size=0.3, \n",
    "                                                  random_state=35)\n",
    "\n",
    "# One hot encoding of the classes\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)#\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "# Image Data Generator for preprocessing and data augmentation\n",
    "if(data_augmentation==True):\n",
    "    train_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input_image,\n",
    "                                            width_shift_range=0.1,\n",
    "                                            height_shift_range=0.1,\n",
    "                                            shear_range=0.1,\n",
    "                                            rotation_range=8,\n",
    "                                            fill_mode='nearest'\n",
    "                                            )\n",
    "else:\n",
    "    train_datagen = image.ImageDataGenerator()\n",
    "    \n",
    "train_generator = train_datagen.flow(x=X_train, y=y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model building\n",
    "\n",
    "I use the VGG16 implementation and I remove the last layer to add a layer that suits my needs. In this case it is a 10 Fully connected layer so that I can use the CNN as a 10-class classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16_model(n_classes=10, n_layers_train=2, learning_rate=0.0001):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - n_classes: number of classes to predict for the classifier\n",
    "    - n_layers_train: number of last layers to train. The other layers will be frozen.\n",
    "    Return:\n",
    "    - model: Keras model\n",
    "    \"\"\"\n",
    "    #Load the VGG model\n",
    "    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=True, input_shape=(224,224,3))    \n",
    "\n",
    "    # Remove top layer and connect custom dense layer\n",
    "    fc2 = vgg16_base.get_layer('fc2').output\n",
    "    mypredictions = Dense(n_classes, activation='softmax', name='mypredictions')(fc2)\n",
    "    model = Model(inputs=vgg16_base.input, outputs=mypredictions)\n",
    "    \n",
    "    model.summary()\n",
    "    # Freeze the layers except the last n_layers_train layers\n",
    "    for layer in model.layers[:-n_layers_train]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[:]:\n",
    "        print(layer, layer.trainable)    \n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizers.Adam(lr=learning_rate),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 39/109 [=========>....................] - ETA: 2:12 - loss: 1.9631 - acc: 0.7459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-101:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Process ForkPoolWorker-102:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-7f1348f1da00>\", line 13, in <module>\n",
      "    use_multiprocessing=True)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training.py\", line 1426, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 191, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training.py\", line 1220, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2661, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2631, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1451, in __call__\n",
      "    self._session._session, self._handle, args, status, None)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/posixpath.py\", line 408, in _joinrealpath\n",
      "    name, _, rest = rest.partition(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Tensorboard logs\n",
    "tbCallBack = callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "          write_graph=False, write_images=False)\n",
    "\n",
    "# Fit the model on batches with real-time data augmentation:\n",
    "model_history = model.fit_generator(train_generator,\n",
    "                                    validation_data=(X_val, y_val),\n",
    "                                    shuffle=True,\n",
    "                                    epochs=n_epoch,\n",
    "                                    steps_per_epoch=np.ceil(X_train.shape[0]//batch_size),\n",
    "                                    callbacks=[tbCallBack],\n",
    "                                    verbose=1,\n",
    "                                    use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can save it for further predictions.\n",
    "\n",
    "Use the script `predict.py [Model/model_name]` to obtain predictions and generate a .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and Exit\n",
    "if not os.path.isdir('Model'):\n",
    "    os.mkdir('Model')\n",
    "filename = 'VGG16_lr'+str(learning_rate)+'_train'+str(n_layers_train)+'_epochs'+str(n_epoch)+'.h5'\n",
    "\n",
    "model_file = os.path.join('Model', filename)\n",
    "model.save(model_file)\n",
    "print('File', filename, 'saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict and submit\n",
    "\n",
    "Using the model that was trained in this notebook, the script is going to predict the results for the test set and directly submit the results to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#delete when no longer needed\n",
    "del X_train, X_val, X_train_, model_history\n",
    "#collect residual garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file='Model/VGG16_lr0.0001_train3_epochs5.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'predict.py' $model_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#delete when no longer needed\n",
    "del X_test, model\n",
    "#collect residual garbage\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c state-farm-distracted-driver-detection -f \"$(ls -t Output/submission-* | head -1)\" -m \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

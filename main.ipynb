{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect distracted drivers\n",
    "*Cyril Pecoraro - June 1st 2018*\n",
    "\n",
    "The goal is to predict what the driver is doing based on pictures\n",
    "\n",
    "This project is from a [Kaggle competition : State Farm Distracted Driver Detection\n",
    "](https://www.kaggle.com/c/state-farm-distracted-driver-detection)\n",
    "\n",
    "There are 10 classes to predict:\n",
    "* c0: safe driving\n",
    "* c1: texting - right\n",
    "* c2: talking on the phone - right\n",
    "* c3: texting - left\n",
    "* c4: talking on the phone - left\n",
    "* c5: operating the radio\n",
    "* c6: drinking\n",
    "* c7: reaching behind\n",
    "* c8: hair and makeup\n",
    "* c9: talking to passenger\n",
    "\n",
    "### Explanation of my work\n",
    "\n",
    "\n",
    "### Content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = pd.read_csv('data/driver_imgs_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_image(img_file, img_reshape_size):\n",
    "    \"\"\"Load an image\n",
    "    Args:\n",
    "    - img_file: image file\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    Return:\n",
    "    - img: openCV image\n",
    "    \"\"\"   \n",
    "    \n",
    "    img = cv2.imread(img_file)\n",
    "    img = cv2.resize(img, img_reshape_size)\n",
    "    return img\n",
    "\n",
    "def load_dataset(dataset_dir, X, y, img_reshape_size, train_dir=True, nprocs=10):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - dataset_dir: path to the directory containing subdirectories of images\n",
    "    - X: list to append data\n",
    "    - y: list to append class\n",
    "    - img_reshape_size: shape(w,h) to resize the image\n",
    "    - nprocs:Number of processors to use\n",
    "    Return:\n",
    "    - X: list with each image data as a row\n",
    "    - y: list with each class as an integer for each image\n",
    "    \"\"\"\n",
    "    # Train dataset\n",
    "    if (train_dir): \n",
    "        for i in range(10):\n",
    "            path = os.path.join(dataset_dir, 'c'+str(i),'*.jpg')\n",
    "            files = glob.glob(path)\n",
    "\n",
    "            X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "            y.extend([i]*len(files))\n",
    "            print('folder train/c'+str(i), 'loaded')\n",
    "            \n",
    "    # Test dataset\n",
    "    else: \n",
    "        path = os.path.join(dataset_dir, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "\n",
    "        X.extend(Parallel(n_jobs=nprocs)(delayed(load_image)(im_file, img_reshape_size) for im_file in files))\n",
    "        print('folder test loaded')\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def convert_to_one_hot(Y, C=10):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset train\n",
      "folder train/c 0 loaded\n",
      "folder train/c 1 loaded\n",
      "folder train/c 2 loaded\n",
      "folder train/c 3 loaded\n",
      "folder train/c 4 loaded\n",
      "folder train/c 5 loaded\n",
      "folder train/c 6 loaded\n",
      "folder train/c 7 loaded\n",
      "folder train/c 8 loaded\n",
      "folder train/c 9 loaded\n"
     ]
    }
   ],
   "source": [
    "# Working directories#  \n",
    "dataset_dir = 'data'\n",
    "dataset_dir_train = os.path.join(dataset_dir, 'train')\n",
    "dataset_dir_test = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# Image sizes\n",
    "img_reshape_size = (224,224)\n",
    "\n",
    "# Load train dataset\n",
    "print('Load dataset train')\n",
    "X_train_ = []\n",
    "y_train_ = []\n",
    "X_train_, y_train_ = load_dataset(dataset_dir_train, X_train_, y_train_, img_reshape_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15696, 224, 224, 3)\n",
      "y_train shape: (15696, 10)\n",
      "X_val shape: (6728, 224, 224, 3)\n",
      "y_val shape: (6728, 10)\n"
     ]
    }
   ],
   "source": [
    "# Creation of a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42)\n",
    "\n",
    "# Preprocess input according to VGG16\n",
    "vgg16.preprocess_input(X_train)\n",
    "vgg16.preprocess_input(X_val)\n",
    "\n",
    "# One-hot encoding of the target vector\n",
    "y_train = convert_to_one_hot(y_train)\n",
    "y_val = convert_to_one_hot(y_val)\n",
    "\n",
    "\n",
    "# Shapes \n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_VGG16_model(n_classes=10, n_layers_freeze=1, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Load the images located in the main folder dataset_dir Each class is in a separate subfolder\n",
    "    Args:\n",
    "    - n_classes: number of classes to predict for the classifier\n",
    "    - n_layers_freeze: number of last layers to freeze so that they are not trained again\n",
    "    Return:\n",
    "    - model: Keras model\n",
    "    \"\"\"\n",
    "    #Load the VGG model\n",
    "    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze the layers except the last n_layers_freeze layers\n",
    "    for layer in vgg16_base.layers[:-n_layers_freeze]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential(name='VGG16-classifier')\n",
    "    model.add(vgg16_base)\n",
    "    model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                250890    \n",
      "=================================================================\n",
      "Total params: 14,965,578\n",
      "Trainable params: 4,970,506\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_epoch = 2\n",
    "\n",
    "model = create_VGG16_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9086172009200653865\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15696 samples, validate on 6728 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-e028a49d906f>\", line 6, in <module>\n",
      "    validation_data=(X_val, y_val),\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training.py\", line 1042, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2661, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2631, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1451, in __call__\n",
      "    self._session._session, self._handle, args, status, None)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/posixpath.py\", line 389, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/posixpath.py\", line 378, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/home/peco/miniconda3/envs/myenv/lib/python3.6/posixpath.py\", line 355, in normpath\n",
      "    if comp in (empty, dot):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(x=X_train, y=y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=n_epoch,\n",
    "          shuffle=True,\n",
    "          verbose=10,\n",
    "          validation_data=(X_val, y_val),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 400/6728 [>.............................] - ETA: 24:24"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cfeff8b8f16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions_valid = model.predict(X_val, batch_size=batch_size, verbose=1)\n",
    "score = log_loss(Y_val, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a2c210ec87cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = model_history.history['acc']\n",
    "val_acc = model_history.history['val_acc']\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
